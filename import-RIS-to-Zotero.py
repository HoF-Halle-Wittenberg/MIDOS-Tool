#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Zotero RIS Import Script
------------------------
Dieses Skript importiert RIS-Dateien in eine Zotero-Gruppensammlung.

Features:
- Robuste Fehlerbehandlung und Retry-Logik
- Intelligenter Fallback-Parser bei Zotero-Server-√úberlastung
- Automatische Erkennung und korrekte Verarbeitung von Sammelb√§nden
- Detaillierte Logging und Fortschrittsanzeige
- Batch-Upload mit konfigurierbarer Gr√∂√üe

Version: 1.0
Datum: 2025-07-15
Lizenz: MIT
"""

import requests
import json
import time
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import os

# Wechsle zum Verzeichnis des Skripts, um relative Pfade zu vereinfachen
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)

# Robustes Logging konfigurieren
def setup_logging():
    """Logging sicher konfigurieren mit Fehlerbehandlung"""
    
    # Logger erstellen
    logger = logging.getLogger(__name__)
    
    # Vermeide doppelte Handler
    if logger.handlers:
        return logger
        
    logger.setLevel(logging.INFO)
    
    # Formatter
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    # Console Handler (immer funktionierend)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # File Handler mit Fehlerbehandlung
    try:
        log_filename = f'zotero_import_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        file_handler = logging.FileHandler(log_filename, mode='w', encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        # Test-Schreibung
        logger.info(f"üìù Log-Datei erstellt: {log_filename}")
        file_handler.flush()  # Sofort schreiben
        
    except (PermissionError, OSError) as e:
        logger.warning(f"‚ö†Ô∏è  Konnte Log-Datei nicht erstellen: {e}")
        logger.warning("üì∫ Ausgabe nur in Konsole")
    
    return logger

# Logger initialisieren
logger = setup_logging()

class ZoteroImporter:
    def __init__(self, group_id: str, api_key: str):
        self.group_id = group_id
        self.api_key = api_key
        self.translation_servers = [
            "https://translate.zotero.org/web",
            "https://translate.zotero.org/web",  # Backup (same server, but for retry)
        ]
        self.chunk_size = 100  # Anzahl RIS-Eintr√§ge pro Translation-Request
        self.use_fallback_parser = True  # Manuellen Parser bei Server-Problemen verwenden
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ZoteroImporter/1.0 (Python)'
        })
        self.skip_translation_server = True  
        
    def validate_ris_content(self, ris_content: str) -> Tuple[bool, str]:
        """RIS-Inhalt validieren"""
        if not ris_content.strip():
            return False, "RIS-Datei ist leer"
        
        if not any(line.startswith('TY  -') for line in ris_content.split('\n')):
            return False, "Keine g√ºltigen RIS-Eintr√§ge gefunden (TY-Tag fehlt)"
        
        # Z√§hle Eintr√§ge
        entries = ris_content.count('TY  -')
        if entries == 0:
            return False, "Keine RIS-Eintr√§ge gefunden"
        
        logger.info(f"‚úì RIS-Datei validiert: {entries} Eintr√§ge gefunden")
        return True, f"{entries} Eintr√§ge gefunden"

    def split_ris_content(self, ris_content: str, chunk_size: int = 100) -> List[str]:
        """RIS-Content in kleinere Chunks aufteilen"""
        entries = []
        current_entry = []
        
        for line in ris_content.split('\n'):
            if line.startswith('TY  -') and current_entry:
                entries.append('\n'.join(current_entry))
                current_entry = [line]
            else:
                current_entry.append(line)
        
        if current_entry:
            entries.append('\n'.join(current_entry))
        
        # Entries in Chunks aufteilen
        chunks = []
        for i in range(0, len(entries), chunk_size):
            chunk = entries[i:i+chunk_size]
            chunks.append('\n'.join(chunk))
        
        logger.info(f"RIS aufgeteilt in {len(chunks)} Chunks √† max. {chunk_size} Eintr√§ge")
        return chunks

    def parse_ris_manually(self, ris_content: str) -> List[Dict]:
        """
        Vollst√§ndiger manueller RIS-Parser mit umfassendem Logging
        Jetzt mit Sammelband-Unterst√ºtzung
        """
        logger.info("üîß FALLBACK PARSER AKTIVIERT")
        logger.info("=" * 50)
        
        # Eingabe-Statistiken
        lines = ris_content.split('\n')
        entry_count = ris_content.count('TY  -')
        logger.info(f"üìÑ Analysiere {len(lines)} Zeilen")
        logger.info(f"üìö Erkannte RIS-Eintr√§ge: {entry_count}")
        logger.info(f"‚ö° Beginne manuelles Parsing...")
        logger.info("-" * 50)
        
        items = []
        current_item = None
        processed_entries = 0
        
        # Statistiken
        item_types = {}
        creator_stats = {'authors': 0, 'editors': 0, 'contributors': 0}
        field_stats = {}
        
        # Zotero Item-Type spezifische Felder (erweitert mit book f√ºr Sammelb√§nde)
        valid_fields = {
            'journalArticle': ['title', 'creators', 'publicationTitle', 'volume', 'issue', 'pages', 'date', 'ISSN', 'url', 'abstractNote', 'tags', 'DOI', 'language', 'extra', 'callNumber'],
            'book': ['title', 'creators', 'publisher', 'place', 'date', 'ISBN', 'url', 'abstractNote', 'tags', 'language', 'numPages', 'series', 'seriesNumber', 'edition', 'extra', 'callNumber'],
            'bookSection': ['title', 'creators', 'bookTitle', 'publisher', 'place', 'date', 'pages', 'ISBN', 'url', 'abstractNote', 'tags', 'language', 'series', 'seriesNumber', 'edition', 'extra', 'callNumber'],
            'conferencePaper': ['title', 'creators', 'proceedingsTitle', 'place', 'date', 'pages', 'url', 'abstractNote', 'tags', 'DOI', 'language', 'conferenceName', 'extra', 'callNumber'],
            'thesis': ['title', 'creators', 'university', 'place', 'date', 'thesisType', 'url', 'abstractNote', 'tags', 'language', 'extra', 'callNumber'],
            'report': ['title', 'creators', 'institution', 'place', 'date', 'reportNumber', 'url', 'abstractNote', 'tags', 'language', 'reportType', 'extra', 'callNumber'],
            'webpage': ['title', 'creators', 'websiteTitle', 'url', 'accessDate', 'abstractNote', 'tags', 'language', 'extra'],
            'newspaperArticle': ['title', 'creators', 'publicationTitle', 'place', 'date', 'pages', 'url', 'abstractNote', 'tags', 'language', 'section', 'edition', 'extra', 'callNumber'],
            'magazineArticle': ['title', 'creators', 'publicationTitle', 'date', 'pages', 'ISSN', 'url', 'abstractNote', 'tags', 'language', 'extra', 'callNumber'],
            'document': ['title', 'creators', 'publisher', 'date', 'url', 'abstractNote', 'tags', 'language', 'extra', 'callNumber'],
            'manuscript': ['title', 'creators', 'place', 'date', 'manuscriptType', 'url', 'abstractNote', 'tags', 'language', 'extra', 'callNumber'],
            'presentation': ['title', 'creators', 'presentationType', 'place', 'date', 'url', 'abstractNote', 'tags', 'language', 'extra'],
            'patent': ['title', 'creators', 'country', 'assignee', 'patentNumber', 'priorityNumbers', 'date', 'url', 'abstractNote', 'tags', 'language', 'extra'],
            'computerProgram': ['title', 'creators', 'company', 'place', 'date', 'programmingLanguage', 'system', 'url', 'abstractNote', 'tags', 'language', 'extra'],
            'audioRecording': ['title', 'creators', 'label', 'place', 'date', 'runningTime', 'url', 'abstractNote', 'tags', 'language', 'extra'],
            'videoRecording': ['title', 'creators', 'studio', 'place', 'date', 'runningTime', 'url', 'abstractNote', 'tags', 'language', 'extra']
        }
        
        # Erweiterte RIS zu Zotero Feld-Mapping (mit T4 und H2)
        field_mapping = {
            # Standard Felder
            'TY': 'itemType',
            'TI': 'title', 
            'T1': 'title',
            'T2': 'publicationTitle',  # Journal/Book title
            'T3': 'series',           # Series title
            'T4': 'subtitle',         # Untertitel (wird mit Titel zusammengef√ºhrt)
            'AU': 'creators',         # Author
            'A1': 'creators',         # Primary Author
            'A2': 'creators',         # Secondary Author (Editor)
            'A3': 'creators',         # Tertiary Author
            'ED': 'creators',         # Editor
            'PY': 'date',
            'Y1': 'date',
            'DA': 'date',
            'JO': 'publicationTitle', # Journal abbreviation
            'JF': 'publicationTitle', # Journal full name
            'JA': 'publicationTitle', # Journal abbreviation
            'VL': 'volume',
            'IS': 'issue',
            'SP': 'start_page',       # Start page (wird sp√§ter kombiniert)
            'EP': 'end_page',         # End page (wird sp√§ter kombiniert)
            'PB': 'publisher',
            'CY': 'place',
            'SN': 'ISSN',             # ISSN/ISBN
            'BN': 'ISBN',             # ISBN
            'UR': 'url',
            'L1': 'url',              # Link to PDF
            'L2': 'url',              # Link to Full Text
            'AB': 'abstractNote',
            'N2': 'abstractNote',
            'N1': 'extra',            # Notes/Extra info
            'KW': 'tags',
            'DO': 'DOI',
            'LA': 'language',
            'CN': 'callNumber',       # Call Number/Signatur
            'H2': 'callNumber',       # Zus√§tzliche Signatur (Alternative zu CN)
            'M1': 'extra',            # Miscellaneous 1
            'M2': 'extra',            # Miscellaneous 2
            'M3': 'extra',            # Miscellaneous 3
            'AD': 'extra',            # Author Address
            'AN': 'extra',            # Accession Number
            'AV': 'extra',            # Availability
            'C1': 'extra',            # Custom 1
            'C2': 'extra',            # Custom 2
            'C3': 'extra',            # Custom 3
            'CA': 'extra',            # Caption
            'DB': 'extra',            # Database
            'DP': 'extra',            # Database Provider
            'ET': 'edition',          # Edition
            'ID': 'extra',            # Reference ID
            'IP': 'issue',            # Issue
            'NV': 'seriesNumber',     # Number of Volumes
            'OP': 'extra',            # Original Publication
            'PP': 'place',            # Place Published
            'RP': 'extra',            # Reprint Edition
            'SE': 'section',          # Section
            'ST': 'shortTitle',       # Short Title
            'TA': 'extra',            # Translated Author
            'TT': 'extra',            # Translated Title
            'U1': 'extra',            # User definable 1
            'U2': 'extra',            # User definable 2
            'U3': 'extra',            # User definable 3
            'U4': 'extra',            # User definable 4
            'U5': 'extra',            # User definable 5
            'Y2': 'accessDate',       # Access Date
        }
        
        # Item-Type Mapping (erweitert mit SAMMELBAND)
        type_mapping = {
            'JOUR': 'journalArticle',
            'BOOK': 'book',
            'CHAP': 'bookSection',
            'CONF': 'conferencePaper',
            'THES': 'thesis',
            'RPRT': 'report',
            'WEB': 'webpage',
            'NEWS': 'newspaperArticle',
            'MGZN': 'magazineArticle',
            'ABST': 'journalArticle',
            'ADVS': 'audiovisualMaterial',
            'AGGR': 'journalArticle',
            'ANCIENT': 'manuscript',
            'ART': 'artwork',
            'BILL': 'bill',
            'BLOG': 'blogPost',
            'CASE': 'case',
            'CTLG': 'catalog',
            'DATA': 'dataset',
            'DBASE': 'computerProgram',
            'DICT': 'dictionaryEntry',
            'EBOOK': 'book',
            'ECHAP': 'bookSection',
            'EDBOOK': 'book',
            'EJOUR': 'journalArticle',
            'ELEC': 'document',
            'ENCYC': 'encyclopediaArticle',
            'EQUA': 'equation',
            'FIGURE': 'figure',
            'GEN': 'report',          
            'GOVDOC': 'report',       
            'GRANT': 'document',
            'HEAR': 'hearing',
            'ICOMM': 'document',
            'INPR': 'document',
            'JFULL': 'journalArticle',
            'LEGAL': 'document',
            'MANSCPT': 'manuscript',
            'MAP': 'map',
            'MULTI': 'document',
            'MUSIC': 'audioRecording',
            'PAMP': 'document',
            'PAT': 'patent',
            'PCOMM': 'letter',
            'SLIDE': 'presentation',
            'SOUND': 'audioRecording',
            'STAND': 'document',
            'STAT': 'statute',
            'UNBILL': 'bill',
            'UNPB': 'document',
            'VIDEO': 'videoRecording',
            # NEUE SAMMELB√ÑNDE
            'SAMMELBAND': 'book',     # Sammelband -> book (herausgegebenes Buch)
            'SAMMLUNG': 'book',       # Alternative deutsche Bezeichnung
            'EDITED': 'book',         # Alternative englische Bezeichnung
            'ANTHOLOGY': 'book'       # Anthologie/Sammelwerk
        }
        
        for line_num, line in enumerate(lines):
            line = line.strip()
            
            if line.startswith('TY  -'):
                # Neuer Eintrag
                if current_item:
                    items.append(self._finalize_item(current_item, valid_fields))
                    
                processed_entries += 1
                
                # Progress-Updates
                if processed_entries % 50 == 0:
                    logger.info(f"‚è≥ Fallback Progress: {processed_entries}/{entry_count} Eintr√§ge verarbeitet...")
                elif processed_entries % 10 == 0:
                    # K√ºrzere Updates f√ºr kleine Dateien
                    if entry_count < 100:
                        logger.info(f"‚è≥ Parsing: {processed_entries}/{entry_count}")
                
                # Item initialisieren
                current_item = {
                    'creators': [], 
                    'tags': [], 
                    'extra_info': [],
                    'subtitle': None,  # F√ºr T4 Untertitel
                    'is_sammelband': False  # Flag f√ºr Sammelband-Erkennung
                }
                
                # Item Type setzen
                ris_type = line.split('TY  - ', 1)[1].strip()
                zotero_type = type_mapping.get(ris_type, 'journalArticle')
                current_item['itemType'] = zotero_type
                
                # Sammelband-Flag setzen
                if ris_type in ['SAMMELBAND', 'SAMMLUNG', 'EDITED', 'ANTHOLOGY']:
                    current_item['is_sammelband'] = True
                    logger.debug(f"üîñ Sammelband erkannt: {ris_type}")
                
                # Statistiken
                item_types[zotero_type] = item_types.get(zotero_type, 0) + 1
                
            elif line.startswith('ER  -'):
                # Eintrag Ende
                if current_item:
                    items.append(self._finalize_item(current_item, valid_fields))
                    current_item = None
                    
            elif '  - ' in line and current_item is not None:
                # Feld parsen
                try:
                    tag, value = line.split('  - ', 1)
                    value = value.strip()
                    
                    if not value:
                        continue
                    
                    # Statistiken
                    field_stats[tag] = field_stats.get(tag, 0) + 1

                    # C3 direkt f√ºr Pages-Verarbeitung speichern
                    if tag == 'C3':
                        current_item['C3'] = value
                        continue
                        
                    zotero_field = field_mapping.get(tag)
                    if not zotero_field:
                        # Unbekannte Felder in Extra sammeln
                        current_item['extra_info'].append(f"{tag}: {value}")
                        continue
                        
                    if zotero_field == 'creators':
                        # Creator-Type basierend auf RIS-Tag bestimmen
                        # Bei Sammelb√§nden: A2/ED werden standardm√§√üig als Herausgeber behandelt
                        creator_type = 'author'
                        if tag in ['A2', 'ED']:
                            creator_type = 'editor'
                            creator_stats['editors'] += 1
                        elif tag == 'A3':
                            creator_type = 'contributor'
                            creator_stats['contributors'] += 1
                        else:
                            # Bei Sammelb√§nden: AU/A1 k√∂nnen auch Herausgeber sein
                            if current_item.get('is_sammelband', False):
                                # F√ºr Sammelb√§nde: erste Creators als Herausgeber behandeln
                                # es sei denn, es sind schon explizite Herausgeber vorhanden
                                existing_editors = [c for c in current_item['creators'] if c.get('creatorType') == 'editor']
                                if not existing_editors:
                                    creator_type = 'editor'
                                    creator_stats['editors'] += 1
                                else:
                                    creator_stats['authors'] += 1
                            else:
                                creator_stats['authors'] += 1
                        
                        # Name parsen (LastName, FirstName Format)
                        if ',' in value:
                            parts = value.split(',', 1)
                            lastName = parts[0].strip()
                            firstName = parts[1].strip() if len(parts) > 1 else ''
                            current_item['creators'].append({
                                'creatorType': creator_type,
                                'lastName': lastName,
                                'firstName': firstName
                            })
                        else:
                            current_item['creators'].append({
                                'creatorType': creator_type,
                                'name': value
                            })
                            
                    elif zotero_field == 'tags':
                        # Tag hinzuf√ºgen
                        current_item['tags'].append({'tag': value})
                        
                    elif zotero_field == 'extra':
                        # Extra-Info sammeln
                        current_item['extra_info'].append(value)
                        
                    elif zotero_field == 'title':
                        # Titel-Behandlung: T1/TI mit eventuell vorhandenem T4 zusammenf√ºhren
                        existing_subtitle = current_item.get('subtitle')
                        if existing_subtitle:
                            # T4 war bereits da, zusammenf√ºhren
                            current_item['title'] = f"{value}. {existing_subtitle}"
                            current_item.pop('subtitle', None)  # Subtitle nicht mehr n√∂tig
                        else:
                            # Normaler Titel ohne Untertitel
                            current_item['title'] = value
                            
                    elif zotero_field == 'subtitle':
                        # T4 Untertitel f√ºr sp√§teren Merge mit Titel
                        existing_title = current_item.get('title')
                        if existing_title:
                            # T1/TI war bereits da, zusammenf√ºhren
                            current_item['title'] = f"{existing_title}. {value}"
                            # Subtitle wird nicht gesetzt, da bereits zusammengef√ºhrt
                        else:
                            # T4 kommt vor T1, zwischenspeichern
                            current_item['subtitle'] = value
                        
                    elif zotero_field == 'start_page':
                        current_item['start_page'] = value
                    elif zotero_field == 'end_page':
                        current_item['end_page'] = value
                        
                    elif zotero_field == 'date':
                        # Datum normalisieren (YYYY/MM/DD oder YYYY)
                        if value and not current_item.get('date'):
                            # Nur Jahr extrahieren falls komplexes Datum
                            import re
                            year_match = re.search(r'\b(19|20)\d{2}\b', value)
                            if year_match:
                                current_item['date'] = year_match.group()
                            else:
                                current_item['date'] = value
                                
                    else:
                        # Normales Feld
                        current_item[zotero_field] = value
                        
                except Exception as e:
                    logger.warning(f"Fehler beim Parsen der Zeile {line_num}: '{line}' - {e}")
                    continue
        
        # Letzten Eintrag hinzuf√ºgen
        if current_item:
            items.append(self._finalize_item(current_item, valid_fields))
        
        # Detaillierte Abschluss-Statistiken
        sammelb√§nde_count = sum(1 for item in items if item.get('_was_sammelband', False))
        
        logger.info("-" * 50)
        logger.info("üéØ FALLBACK PARSER ABGESCHLOSSEN")
        logger.info(f"‚úÖ Erfolgreich geparst: {len(items)} Items")
        logger.info(f"üìö Davon Sammelb√§nde: {sammelb√§nde_count}")
        logger.info(f"üìä Item-Typen: {dict(sorted(item_types.items()))}")
        logger.info(f"üë• Creators: Autoren={creator_stats['authors']}, Herausgeber={creator_stats['editors']}, Mitwirkende={creator_stats['contributors']}")
        
        # Top 10 h√§ufigste RIS-Felder
        top_fields = sorted(field_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        logger.info(f"üè∑Ô∏è  H√§ufigste RIS-Felder: {dict(top_fields)}")
        
        logger.info("=" * 50)
        
        return items

    def _finalize_item(self, item: Dict, valid_fields: Dict) -> Dict:
        """Item finalisieren und f√ºr Zotero validieren - mit Sammelband-Behandlung"""
        item_type = item.get('itemType', 'journalArticle')
        allowed_fields = valid_fields.get(item_type, valid_fields['journalArticle'])
        is_sammelband = item.pop('is_sammelband', False)
        
        # Titel mit Untertitel zusammenf√ºhren (T4)
        title = item.get('title', '')
        subtitle = item.pop('subtitle', None)
        if title and subtitle:
            # Format: "Titel. Untertitel"
            item['title'] = f"{title}. {subtitle}"
        elif subtitle and not title:
            # Falls nur Untertitel vorhanden
            item['title'] = subtitle
        
        # Seiten korrekt zusammenf√ºgen
        start_page = item.pop('start_page', None)
        end_page = item.pop('end_page', None)
        c3_info = item.pop('C3', None)  # C3 f√ºr erweiterte Seiten-Info

        if 'pages' in allowed_fields or 'numPages' in allowed_fields:
            page_parts = []
            
            # SP analysieren: unterscheiden zwischen "XVI, 198" und "1-22"
            if start_page:
                if ',' in start_page:
                    # Format "XVI, 198" -> komplett in numPages
                    if 'numPages' in allowed_fields:
                        item['numPages'] = start_page  # Komplette Angabe "XVI, 198"
                        
                    # C3 zu pages hinzuf√ºgen falls vorhanden
                    if c3_info and 'pages' in allowed_fields:
                        item['pages'] = c3_info
                else:
                    # Normale Seitenangabe "1-22"
                    if c3_info:
                        page_parts.append(c3_info)
                    page_parts.append(start_page)
                    
                    if 'pages' in allowed_fields and page_parts:
                        item['pages'] = ', '.join(page_parts)
            
            elif end_page and 'pages' in allowed_fields:
                if c3_info:
                    item['pages'] = f"{c3_info}, {end_page}"
                else:
                    item['pages'] = end_page
        
    
        # Extra-Informationen zusammenfassen
        extra_info = item.pop('extra_info', [])
        
        # Sammelband-Info zu Extra hinzuf√ºgen
        if is_sammelband:
            extra_info.insert(0, "Type: Sammelband")
            item['_was_sammelband'] = True  # Flag f√ºr Statistiken
        
        if extra_info and 'extra' in allowed_fields:
            unique_parts = list(dict.fromkeys(extra_info))  # Deduplizieren
            existing_extra = item.get('extra', '')
            if existing_extra:
                unique_parts.insert(0, existing_extra)
            
            # " | " zu Zeilenumbr√ºchen konvertieren f√ºr bessere Lesbarkeit
            combined_extra = '\n'.join(unique_parts)
            combined_extra = combined_extra.replace(' | ', '\n')
            item['extra'] = combined_extra
        
        # Feld-Anpassungen basierend auf Item-Type
        if item_type == 'bookSection':
            if 'publicationTitle' in item:
                item['bookTitle'] = item.pop('publicationTitle')
            item.pop('ISSN', None)
            item.pop('volume', None)
            item.pop('issue', None)
            
        elif item_type == 'book':
            # F√ºr Sammelb√§nde: spezielle Behandlung
            if is_sammelband:
                # Bei Sammelb√§nden: sicherstellen, dass Herausgeber korrekt gesetzt sind
                creators = item.get('creators', [])
                has_editors = any(c.get('creatorType') == 'editor' for c in creators)
                
                # Falls keine expliziten Herausgeber: erste Creators zu Herausgebern machen
                if not has_editors and creators:
                    for creator in creators:
                        if creator.get('creatorType') == 'author':
                            creator['creatorType'] = 'editor'
                            logger.debug(f"üìù Autor zu Herausgeber ge√§ndert: {creator.get('lastName', creator.get('name', ''))}")
            
            if 'ISSN' in item:
                item['ISBN'] = item.pop('ISSN')
            item.pop('publicationTitle', None)
            item.pop('volume', None)
            item.pop('issue', None)
            item.pop('pages', None)
            
        elif item_type == 'journalArticle':
            item.pop('place', None)
            item.pop('publisher', None)
            
        elif item_type in ['document', 'report']:
            # F√ºr document/report: ung√ºltige Felder in Extra verschieben
            invalid_fields = ['place', 'series', 'seriesNumber', 'pages', 'volume', 'issue', 'ISSN', 'ISBN', 'DOI']
            extra_additions = []
            
            for field in invalid_fields:
                if field in item:
                    value = item.pop(field)
                    if field == 'place':
                        extra_additions.append(f"Place: {value}")
                    elif field == 'series':
                        extra_additions.append(f"Series: {value}")
                    elif field == 'pages':
                        extra_additions.append(f"Pages: {value}")
                    elif field == 'DOI':
                        extra_additions.append(f"DOI: {value}")
                    else:
                        extra_additions.append(f"{field}: {value}")
            
            if extra_additions:
                existing_extra = item.get('extra', '')
                if existing_extra:
                    extra_additions.insert(0, existing_extra)
                item['extra'] = '\n'.join(extra_additions)
            
        elif item_type == 'conferencePaper':
            if 'publicationTitle' in item:
                item['proceedingsTitle'] = item.pop('publicationTitle')
            item.pop('ISSN', None)
            item.pop('volume', None)
            item.pop('issue', None)
        
        # Nur erlaubte Felder behalten
        cleaned_item = {}
        for key, value in item.items():
            if key in allowed_fields or key in ['itemType', 'creators', 'tags', '_was_sammelband']:
                cleaned_item[key] = value
        
        # Leere Arrays entfernen
        if not cleaned_item.get('creators'):
            cleaned_item.pop('creators', None)
        if not cleaned_item.get('tags'):
            cleaned_item.pop('tags', None)
            
        return cleaned_item

    def convert_ris_with_fallback(self, ris_content: str) -> Optional[List[Dict]]:
        """RIS konvertieren mit intelligentem Fallback-System - SOFORT FALLBACK"""
        
        # SOFORT FALLBACK: Server sind oft √ºberlastet
        if self.use_fallback_parser:
            logger.info("‚ö° DIREKTER FALLBACK aktiviert - √ºberspringe Translation Server")
            logger.info("üîß Verwende sofort den manuellen Parser (zuverl√§ssiger)")
            logger.info("üìä Sie erhalten detaillierte Progress-Updates...")
            
            try:
                return self.parse_ris_manually(ris_content)
            except Exception as e:
                logger.error(f"‚ùå Fallback-Parser fehlgeschlagen: {e}")
                return None
        
        # Optional: Translation Server nur wenn explizit gew√ºnscht
        logger.info("üåê Versuche Zotero Translation Server (oft √ºberlastet)...")
        items = self.convert_ris_with_retry(ris_content, max_retries=1)
        
        if items:
            logger.info(f"‚úÖ Translation Server erfolgreich: {len(items)} Items")
            return items
        else:
            logger.error("‚ùå Translation Server fehlgeschlagen und Fallback deaktiviert")
            return None

    def convert_ris_with_retry(self, ris_content: str, max_retries: int = 5) -> Optional[List[Dict]]:
        """RIS zu Zotero-JSON mit Retry-Logik und intelligentem Fallback"""
        
        # Erst versuchen, alles auf einmal zu konvertieren
        logger.info("Versuche vollst√§ndige Translation...")
        result = self._convert_single_chunk(ris_content, max_retries=2)
        if result:
            return result
        
        # Falls das fehlschl√§gt, in kleinere Chunks aufteilen
        logger.warning("Vollst√§ndige Translation fehlgeschlagen. Teile in Chunks auf...")
        chunks = self.split_ris_content(ris_content, self.chunk_size)
        
        all_items = []
        failed_chunks = 0
        fallback_used = False  # Flag um Schleife zu vermeiden
        
        for i, chunk in enumerate(chunks):
            logger.info(f"Chunk {i+1}/{len(chunks)} ({chunk.count('TY  -')} Eintr√§ge)...")
            
            chunk_items = self._convert_single_chunk(chunk, max_retries)
            if chunk_items:
                all_items.extend(chunk_items)
                logger.info(f"‚úì Chunk {i+1} erfolgreich: {len(chunk_items)} Items")
                failed_chunks = 0  # Reset bei Erfolg
            else:
                failed_chunks += 1
                logger.error(f"‚ùå Chunk {i+1} fehlgeschlagen")
                
                # Schneller Fallback: Nach 2 aufeinanderfolgenden Fehlern
                if failed_chunks >= 2 and not fallback_used:
                    logger.warning("‚ö° 2 Chunks hintereinander fehlgeschlagen - aktiviere sofort Fallback-Parser!")
                    fallback_used = True  # Verhindert weitere Fallback-Versuche
                    
                    remaining_chunks = chunks[max(0, i-1):]  # Sichere Index-Berechnung
                    remaining_content = '\n'.join(remaining_chunks)
                    
                    if self.use_fallback_parser:
                        try:
                            fallback_items = self.parse_ris_manually(remaining_content)
                            if fallback_items:
                                # Entferne bereits erfolgreich verarbeitete Items
                                successful_entries = sum(chunk.count('TY  -') for chunk in chunks[:max(0, i-1)])
                                if successful_entries > 0 and successful_entries < len(fallback_items):
                                    fallback_items = fallback_items[successful_entries:]
                                
                                all_items.extend(fallback_items)
                                logger.info(f"üîß Fallback erfolgreich: {len(fallback_items)} Items hinzugef√ºgt")
                                logger.info("‚úÖ Stoppe Chunk-Verarbeitung - Fallback komplett")
                                break
                            else:
                                logger.error("‚ùå Fallback-Parser gab keine Items zur√ºck")
                        except Exception as e:
                            logger.error(f"‚ùå Fallback-Parser fehlgeschlagen: {e}")
                            logger.error("‚ö†Ô∏è  Setze Chunk-Verarbeitung fort...")
            
            # Pause zwischen Chunks bei Server-√úberlastung
            if i < len(chunks) - 1 and not fallback_used:
                time.sleep(2)
        
        if failed_chunks > 0 and not all_items:
            logger.warning(f"‚ö†Ô∏è  Alle Chunks fehlgeschlagen")
        
        return all_items if all_items else None

    def _convert_single_chunk(self, ris_content: str, max_retries: int = 5) -> Optional[List[Dict]]:
        """Einzelnen RIS-Chunk konvertieren"""
        
        for attempt in range(max_retries):
            for server_idx, server_url in enumerate(self.translation_servers):
                try:
                    wait_time_503 = min(60, 5 * (2 ** attempt))  # Max 60s warten
                    
                    response = self.session.post(
                        server_url,
                        data=ris_content.encode('utf-8'),
                        headers={'Content-Type': 'text/plain'},
                        timeout=120  # L√§ngerer Timeout
                    )
                    
                    if response.status_code == 200:
                        try:
                            items = response.json()
                            if isinstance(items, list) and len(items) > 0:
                                return items
                            else:
                                logger.warning("Translation gab leere Antwort zur√ºck")
                                continue
                        except json.JSONDecodeError as e:
                            logger.error(f"JSON-Parse-Fehler: {e}")
                            continue
                    
                    elif response.status_code == 503:
                        logger.warning(f"Server √ºberlastet (503). Warte {wait_time_503}s... (Versuch {attempt+1})")
                        time.sleep(wait_time_503)
                        continue
                    
                    elif response.status_code == 429:
                        retry_after = int(response.headers.get('Retry-After', 120))
                        logger.warning(f"Rate limit erreicht. Warte {retry_after}s...")
                        time.sleep(retry_after)
                        continue
                    
                    else:
                        logger.error(f"Translation Server Fehler: {response.status_code}")
                        continue
                        
                except requests.exceptions.Timeout:
                    logger.warning(f"Timeout bei Server {server_idx + 1}")
                    continue
                except requests.exceptions.ConnectionError:
                    logger.warning(f"Verbindungsfehler zu Server {server_idx + 1}")
                    time.sleep(5)
                    continue
                except Exception as e:
                    logger.error(f"Unerwarteter Fehler: {e}")
                    continue
            
            # L√§ngere Pause zwischen Versuchen
            if attempt < max_retries - 1:
                wait_time = min(300, 30 * (attempt + 1))  # Max 5 Minuten
                logger.info(f"Alle Server fehlgeschlagen. Warte {wait_time}s vor n√§chstem Versuch...")
                time.sleep(wait_time)
        
        return None

    def get_library_version(self) -> Optional[str]:
        """Aktuelle Library-Version abrufen mit Retry"""
        for attempt in range(3):
            try:
                url = f"https://api.zotero.org/groups/{self.group_id}/items"
                response = self.session.get(
                    url, 
                    headers={"Zotero-API-Key": self.api_key},
                    params={"limit": 1},
                    timeout=30
                )
                
                if response.status_code == 200:
                    version = response.headers.get('Last-Modified-Version')
                    logger.info(f"‚úì Library-Version abgerufen: {version}")
                    return version
                else:
                    logger.warning(f"Fehler beim Abrufen der Library-Version: {response.status_code}")
                    
            except Exception as e:
                logger.warning(f"Versuch {attempt + 1}: {e}")
                time.sleep(2)
        
        logger.error("Konnte Library-Version nicht abrufen")
        return None

    def get_existing_items(self) -> List[Dict]:
        """Alle existierenden Items aus der Zotero-Bibliothek abrufen f√ºr Duplikatspr√ºfung"""
        logger.info("üìö Lade existierende Items f√ºr Duplikatspr√ºfung...")
        all_items = []
        start = 0
        limit = 100
        
        while True:
            try:
                url = f"https://api.zotero.org/groups/{self.group_id}/items"
                params = {
                    "start": start,
                    "limit": limit,
                    "format": "json",
                    "include": "data"
                }
                
                response = self.session.get(
                    url,
                    headers={"Zotero-API-Key": self.api_key},
                    params=params,
                    timeout=60
                )
                
                if response.status_code == 200:
                    items = response.json()
                    if not items:
                        break
                    
                    # Items im vollst√§ndigen Format f√ºr besseren Vergleich speichern
                    for item in items:
                        data = item.get('data', {})
                        # Vollst√§ndiges Item-Objekt f√ºr normalize_item_for_comparison
                        full_item = {
                            'key': data.get('key'),
                            'title': data.get('title', ''),
                            'DOI': data.get('DOI', ''),
                            'ISBN': data.get('ISBN', ''),
                            'ISSN': data.get('ISSN', ''),
                            'creators': data.get('creators', []),
                            'date': data.get('date', ''),
                            'publicationTitle': data.get('publicationTitle', ''),
                            'bookTitle': data.get('bookTitle', ''),  # F√ºr bookSections
                            'volume': data.get('volume', ''),
                            'issue': data.get('issue', ''),
                            'pages': data.get('pages', ''),
                            'itemType': data.get('itemType', ''),
                            'publisher': data.get('publisher', ''),
                            'place': data.get('place', ''),
                            'abstractNote': data.get('abstractNote', ''),
                            'url': data.get('url', ''),
                            'extra': data.get('extra', '')
                        }
                        all_items.append(full_item)
                    
                    start += limit
                    if start % 500 == 0:
                        logger.info(f"   üìñ {start} Items geladen...")
                else:
                    logger.error(f"Fehler beim Laden existierender Items: {response.status_code}")
                    break
                    
            except Exception as e:
                logger.error(f"Fehler beim Laden existierender Items: {e}")
                break
        
        logger.info(f"‚úÖ {len(all_items)} existierende Items geladen")
        return all_items

    def normalize_item_for_comparison(self, item: Dict) -> Dict:
        """Normalisiert ein Item f√ºr den Duplikatsvergleich
        Konvertiert sowohl neue Upload-Items als auch existierende Zotero-Items in ein einheitliches Format"""
        
        # Titel normalisieren
        title = item.get('title', '').lower().strip()
        # Entferne h√§ufige Variationen
        title = title.replace('  ', ' ').replace('\n', ' ').replace('\t', ' ')
        
        # DOI normalisieren
        doi = item.get('DOI', '').lower().strip()
        if doi.startswith('doi:'):
            doi = doi[4:]
        if doi.startswith('http://dx.doi.org/'):
            doi = doi[18:]
        if doi.startswith('https://doi.org/'):
            doi = doi[16:]
        
        # Creators normalisieren
        creators = []
        for creator in item.get('creators', []):
            if 'lastName' in creator:
                name = f"{creator.get('lastName', '')}, {creator.get('firstName', '')}".lower().strip()
            else:
                name = creator.get('name', '').lower().strip()
            # Entferne √ºberfl√ºssige Leerzeichen und Kommas
            name = name.replace('  ', ' ').strip(' ,')
            if name:
                creators.append(name)
        
        # Datum normalisieren - nur Jahr extrahieren
        date = item.get('date', '').strip()
        year = ''
        if date:
            import re
            year_match = re.search(r'\b(19|20)\d{2}\b', date)
            if year_match:
                year = year_match.group()
        
        # Publikationstitel normalisieren
        pub_title = item.get('publicationTitle', '').lower().strip()
        # Auch bookTitle f√ºr bookSections ber√ºcksichtigen
        if not pub_title:
            pub_title = item.get('bookTitle', '').lower().strip()
        
        # ISBN/ISSN normalisieren
        isbn = item.get('ISBN', '').replace('-', '').replace(' ', '').lower()
        issn = item.get('ISSN', '').replace('-', '').replace(' ', '').lower()
        
        # Volume und Issue normalisieren
        volume = item.get('volume', '').strip()
        issue = item.get('issue', '').strip()
        
        # Pages normalisieren
        pages = item.get('pages', '').strip()
        
        return {
            'title': title,
            'doi': doi,
            'creators': creators,
            'year': year,
            'publicationTitle': pub_title,
            'isbn': isbn,
            'issn': issn,
            'volume': volume,
            'issue': issue,
            'pages': pages,
            'itemType': item.get('itemType', '')
        }

    def is_duplicate(self, new_item: Dict, existing_items: List[Dict]) -> Tuple[bool, str]:
        """Verbesserte Duplikatserkennung mit normalisiertem Vergleich
        Konvertiert beide Items in das gleiche Format f√ºr besseren Vergleich"""
        
        # Beide Items normalisieren
        new_normalized = self.normalize_item_for_comparison(new_item)
        
        for existing in existing_items:
            existing_normalized = self.normalize_item_for_comparison(existing)
            
            # 1. DOI-Match (st√§rkster Indikator)
            if (new_normalized['doi'] and existing_normalized['doi'] and 
                new_normalized['doi'] == existing_normalized['doi']):
                return True, f"DOI-Match: {new_normalized['doi']}"
            
            # 2. ISBN-Match (f√ºr B√ºcher)
            if (new_normalized['isbn'] and existing_normalized['isbn'] and 
                new_normalized['isbn'] == existing_normalized['isbn']):
                return True, f"ISBN-Match: {new_normalized['isbn']}"
            
            # 3. Exakter Titel + Jahr + mindestens ein Autor
            if (new_normalized['title'] and existing_normalized['title'] and 
                new_normalized['title'] == existing_normalized['title'] and
                new_normalized['year'] and existing_normalized['year'] and
                new_normalized['year'] == existing_normalized['year']):
                # Pr√ºfe Autor-√úberschneidung
                if new_normalized['creators'] and existing_normalized['creators']:
                    common_creators = set(new_normalized['creators']) & set(existing_normalized['creators'])
                    if common_creators:
                        return True, f"Titel+Jahr+Autor-Match: {new_normalized['title'][:50]}..."
            
            # 4. Sehr √§hnlicher Titel + gleiche Publikation + Jahr
            if (new_normalized['title'] and existing_normalized['title'] and
                new_normalized['publicationTitle'] and existing_normalized['publicationTitle'] and
                new_normalized['publicationTitle'] == existing_normalized['publicationTitle'] and
                new_normalized['year'] and existing_normalized['year'] and
                new_normalized['year'] == existing_normalized['year']):
                title_similarity = self._calculate_similarity(new_normalized['title'], existing_normalized['title'])
                if title_similarity > 0.85:  # 85% √Ñhnlichkeit
                    return True, f"√Ñhnlicher Titel in gleicher Publikation+Jahr: {title_similarity:.1%}"
            
            # 5. Zeitschriftenartikel: Titel + Publikation + Volume + Issue + Pages
            if (new_normalized['itemType'] == 'journalArticle' and existing_normalized['itemType'] == 'journalArticle' and
                new_normalized['title'] and existing_normalized['title'] and
                new_normalized['publicationTitle'] and existing_normalized['publicationTitle'] and
                new_normalized['publicationTitle'] == existing_normalized['publicationTitle']):
                # Pr√ºfe verschiedene Kombinationen
                matches = []
                
                # Exakter Titel
                if new_normalized['title'] == existing_normalized['title']:
                    matches.append("title")
                
                # Volume + Issue
                if (new_normalized['volume'] and existing_normalized['volume'] and
                    new_normalized['volume'] == existing_normalized['volume'] and
                    new_normalized['issue'] and existing_normalized['issue'] and
                    new_normalized['issue'] == existing_normalized['issue']):
                    matches.append("vol+issue")
                
                # Pages
                if (new_normalized['pages'] and existing_normalized['pages'] and
                    new_normalized['pages'] == existing_normalized['pages']):
                    matches.append("pages")
                
                # Jahr
                if (new_normalized['year'] and existing_normalized['year'] and
                    new_normalized['year'] == existing_normalized['year']):
                    matches.append("year")
                
                # Wenn mindestens 2 starke Indikatoren √ºbereinstimmen
                if len(matches) >= 2 and "title" in matches:
                    return True, f"Journal-Match ({'+'.join(matches)}): {new_normalized['title'][:50]}..."
            
            # 6. Buchkapitel: Titel + Buchtitel + Jahr
            if (new_normalized['itemType'] == 'bookSection' and existing_normalized['itemType'] == 'bookSection' and
                new_normalized['title'] and existing_normalized['title'] and
                new_normalized['title'] == existing_normalized['title'] and
                new_normalized['publicationTitle'] and existing_normalized['publicationTitle'] and
                new_normalized['publicationTitle'] == existing_normalized['publicationTitle'] and
                new_normalized['year'] and existing_normalized['year'] and
                new_normalized['year'] == existing_normalized['year']):
                return True, f"Buchkapitel-Match: {new_normalized['title'][:50]}..."
        
        return False, ""

    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """Einfache √Ñhnlichkeitsberechnung basierend auf gemeinsamen W√∂rtern"""
        if not str1 or not str2:
            return 0.0
        
        words1 = set(str1.lower().split())
        words2 = set(str2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0

    def filter_duplicates(self, items: List[Dict], existing_items: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """Filtert Duplikate aus der Item-Liste mit verbesserter Erkennung
        Returns: (unique_items, duplicate_items)"""
        
        logger.info("üîç Pr√ºfe auf Duplikate mit verbesserter Erkennung...")
        logger.info("üîß Verwende normalisierte Feldvergleiche f√ºr bessere Genauigkeit...")
        
        unique_items = []
        duplicate_items = []
        
        # Statistiken f√ºr verschiedene Match-Typen
        match_stats = {
            'DOI-Match': 0,
            'ISBN-Match': 0,
            'Titel+Jahr+Autor-Match': 0,
            '√Ñhnlicher Titel in gleicher Publikation+Jahr': 0,
            'Journal-Match': 0,
            'Buchkapitel-Match': 0
        }
        
        for i, item in enumerate(items):
            is_dup, reason = self.is_duplicate(item, existing_items)
            
            if is_dup:
                duplicate_items.append({
                    'item': item,
                    'reason': reason
                })
                
                # Statistiken aktualisieren
                for match_type in match_stats:
                    if match_type in reason:
                        match_stats[match_type] += 1
                        break
                
                logger.debug(f"   üîÑ Duplikat gefunden: {reason}")
            else:
                unique_items.append(item)
            
            # Progress f√ºr gro√üe Listen
            if (i + 1) % 100 == 0:
                logger.info(f"   üìä {i + 1}/{len(items)} Items gepr√ºft...")
        
        logger.info(f"‚úÖ Duplikatspr√ºfung abgeschlossen:")
        logger.info(f"   üìù Neue Items: {len(unique_items)}")
        logger.info(f"   üîÑ Duplikate gefunden: {len(duplicate_items)}")
        
        # Detaillierte Match-Statistiken
        if duplicate_items:
            logger.info("üìä Duplikat-Typen:")
            for match_type, count in match_stats.items():
                if count > 0:
                    logger.info(f"   - {match_type}: {count}")
        
        # Duplikate-Details loggen
        if duplicate_items:
            logger.info("üîÑ Gefundene Duplikate (erste 10):")
            for i, dup in enumerate(duplicate_items[:10]):
                title = dup['item'].get('title', 'Ohne Titel')[:50]
                logger.info(f"   {i+1}. {title}... ({dup['reason']})")
            if len(duplicate_items) > 10:
                logger.info(f"   ... und {len(duplicate_items) - 10} weitere")
        
        return unique_items, duplicate_items

    def _save_duplicates_report(self, duplicates: List[Dict]):
        """Speichert einen detaillierten Bericht √ºber gefundene Duplikate"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"duplicates_report_{timestamp}.txt"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"DUPLIKATE-BERICHT (Verbesserte Erkennung) - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 80 + "\n\n")
                f.write("VERBESSERUNGEN:\n")
                f.write("- Normalisierte Feldvergleiche (Titel, DOI, ISBN, Autoren)\n")
                f.write("- Bessere Behandlung von Zeitschriftenartikeln (Volume, Issue, Pages)\n")
                f.write("- Spezielle Erkennung f√ºr Buchkapitel\n")
                f.write("- Robuste DOI/ISBN-Normalisierung\n")
                f.write("- Jahresextraktion aus verschiedenen Datumsformaten\n\n")
                f.write(f"Anzahl gefundener Duplikate: {len(duplicates)}\n\n")
                
                # Statistiken nach Match-Typ
                match_types = {}
                for dup in duplicates:
                    reason = dup['reason']
                    match_type = reason.split(':')[0] if ':' in reason else reason
                    match_types[match_type] = match_types.get(match_type, 0) + 1
                
                f.write("DUPLIKAT-TYPEN:\n")
                for match_type, count in sorted(match_types.items()):
                    f.write(f"- {match_type}: {count}\n")
                f.write("\n")
                
                f.write("DETAILLIERTE LISTE:\n")
                f.write("-" * 80 + "\n")
                
                for i, dup in enumerate(duplicates, 1):
                    item = dup['item']
                    reason = dup['reason']
                    
                    f.write(f"\n{i}. {item.get('title', 'Ohne Titel')}\n")
                    f.write(f"   üîç Erkennungsgrund: {reason}\n")
                    f.write(f"   üìö Typ: {item.get('itemType', 'unbekannt')}\n")
                    
                    creators = item.get('creators', [])
                    if creators:
                        author_names = []
                        for c in creators[:3]:
                            if 'lastName' in c:
                                author_names.append(f"{c.get('lastName', '')}, {c.get('firstName', '')}")
                            else:
                                author_names.append(c.get('name', ''))
                        f.write(f"   üë• Autoren: {'; '.join(author_names)}\n")
                    
                    if item.get('date'):
                        f.write(f"   üìÖ Jahr: {item.get('date')}\n")
                    if item.get('DOI'):
                        f.write(f"   üîó DOI: {item.get('DOI')}\n")
                    if item.get('ISBN'):
                        f.write(f"   üìñ ISBN: {item.get('ISBN')}\n")
                    if item.get('publicationTitle'):
                        f.write(f"   üì∞ Publikation: {item.get('publicationTitle')}\n")
                    if item.get('volume') or item.get('issue'):
                        vol_issue = f"Vol. {item.get('volume', '')}" if item.get('volume') else ""
                        if item.get('issue'):
                            vol_issue += f", Issue {item.get('issue')}"
                        if vol_issue:
                            f.write(f"   üìä {vol_issue.strip(', ')}\n")
                    if item.get('pages'):
                        f.write(f"   üìÑ Seiten: {item.get('pages')}\n")
                    
                    f.write("\n")
            
            logger.info(f"üìÑ Detaillierter Duplikate-Bericht gespeichert: {filename}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Konnte Duplikate-Bericht nicht speichern: {e}")

    def test_duplicate_detection(self, ris_content: str) -> Dict:
        """Testet die Duplikatserkennung ohne Upload
        Zeigt detaillierte Statistiken √ºber potenzielle Duplikate"""
        
        logger.info("üß™ DUPLIKAT-TEST GESTARTET")
        logger.info("=" * 50)
        
        # 1. RIS konvertieren
        logger.info("üîÑ Konvertiere RIS-Daten...")
        items = self.convert_ris_with_fallback(ris_content)
        if not items:
            logger.error("‚ùå RIS-Konvertierung fehlgeschlagen")
            return {'success': False, 'error': 'Konvertierung fehlgeschlagen'}
        
        logger.info(f"‚úÖ {len(items)} Items konvertiert")
        
        # 2. Existierende Items laden
        logger.info("üìö Lade existierende Items...")
        existing_items = self.get_existing_items()
        if not existing_items:
            logger.warning("‚ö†Ô∏è  Keine existierenden Items gefunden")
            return {'success': True, 'new_items': len(items), 'duplicates': 0, 'match_rate': 0}
        
        logger.info(f"üìñ {len(existing_items)} existierende Items geladen")
        
        # 3. Duplikatspr√ºfung
        logger.info("üîç F√ºhre Duplikatspr√ºfung durch...")
        unique_items, duplicate_items = self.filter_duplicates(items, existing_items)
        
        # 4. Detaillierte Analyse
        total_items = len(items)
        duplicates_found = len(duplicate_items)
        match_rate = (duplicates_found / total_items * 100) if total_items > 0 else 0
        
        logger.info("=" * 50)
        logger.info("üéØ TEST-ERGEBNISSE:")
        logger.info(f"   üìä Gesamt Items: {total_items}")
        logger.info(f"   üîÑ Duplikate gefunden: {duplicates_found}")
        logger.info(f"   üìù Neue Items: {len(unique_items)}")
        logger.info(f"   üéØ Match-Rate: {match_rate:.1f}%")
        
        if match_rate == 100:
            logger.info("üéâ PERFEKT! Alle Items als Duplikate erkannt!")
        elif match_rate >= 95:
            logger.info("‚úÖ SEHR GUT! Fast alle Items erkannt!")
        elif match_rate >= 80:
            logger.info("‚ö†Ô∏è  GUT: Meiste Items erkannt, aber Verbesserung m√∂glich")
        else:
            logger.info("üö® PROBLEMATISCH: Viele Items nicht als Duplikate erkannt")
        
        # 5. Beispiele f√ºr nicht erkannte Items
        if unique_items:
            logger.info("‚ùì NICHT ERKANNTE ITEMS (erste 5):")
            for i, item in enumerate(unique_items[:5]):
                title = item.get('title', 'Ohne Titel')[:60]
                item_type = item.get('itemType', 'unknown')
                logger.info(f"   {i+1}. [{item_type}] {title}...")
                
                # Zeige verf√ºgbare Felder f√ºr Debugging
                fields = []
                if item.get('DOI'): fields.append(f"DOI: {item['DOI']}")
                if item.get('date'): fields.append(f"Jahr: {item['date']}")
                if item.get('creators'): fields.append(f"Autoren: {len(item['creators'])}")
                if fields:
                    logger.info(f"      {' | '.join(fields)}")
        
        logger.info("=" * 50)
        
        return {
            'success': True,
            'total_items': total_items,
            'duplicates_found': duplicates_found,
            'new_items': len(unique_items),
            'match_rate': match_rate,
            'duplicate_details': duplicate_items[:10]  # Erste 10 f√ºr Details
        }

    def test_ris_file_duplicates(self, file_path: str) -> bool:
        """Testet Duplikatserkennung f√ºr eine RIS-Datei ohne Upload"""
        if not os.path.exists(file_path):
            logger.error(f"‚ùå Datei nicht gefunden: {file_path}")
            return False
        
        logger.info(f"üß™ Teste Duplikatserkennung f√ºr: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                ris_content = f.read()
            
            result = self.test_duplicate_detection(ris_content)
            return result.get('success', False)
        except Exception as e:
            logger.error(f"‚ùå Fehler beim Testen: {e}")
            return False

    def upload_items_batch(self, items: List[Dict], library_version: str, batch_index: int = 0) -> Tuple[bool, str, Optional[str]]:
        """Items-Batch hochladen mit detailliertem Fehlerhandling"""
        
        url = f"https://api.zotero.org/groups/{self.group_id}/items"
        headers = {
            "Zotero-API-Key": self.api_key,
            "Content-Type": "application/json",
            "If-Unmodified-Since-Version": library_version
        }
        
        try:
            response = self.session.post(
                url,
                headers=headers,
                data=json.dumps(items),
                timeout=120  # L√§ngerer Timeout f√ºr Upload
            )
            
            if response.status_code == 200:
                result = response.json()
                new_version = response.headers.get('Last-Modified-Version')
                
                # Erfolgreiche und fehlgeschlagene Items analysieren
                successful = result.get('successful', {})
                unchanged = result.get('unchanged', {})
                failed = result.get('failed', {})
                
                success_count = len(successful) + len(unchanged)
                fail_count = len(failed)
                
                # Fehlgeschlagene Items speichern
                if failed:
                    for item_index, error_info in failed.items():
                        try:
                            item_idx = int(item_index)
                            if item_idx < len(items):
                                failed_item = items[item_idx].copy()
                                failed_item['_error'] = error_info
                                failed_item['_batch_index'] = batch_index
                                failed_item['_item_index'] = item_idx
                                self.failed_items.append(failed_item)
                        except (ValueError, IndexError) as e:
                            logger.warning(f"Konnte fehlgeschlagenes Item nicht zuordnen: {e}")
                
                message = f"‚úì Batch Upload: {success_count} erfolgreich"
                if fail_count > 0:
                    message += f", {fail_count} fehlgeschlagen"
                    logger.warning(f"Fehlgeschlagene Items: {failed}")
                
                return True, message, new_version
                
            elif response.status_code == 412:
                return False, "Library-Version veraltet (412)", None
                
            elif response.status_code == 413:
                return False, "Request zu gro√ü (413) - verkleinern Sie die Batch-Gr√∂√üe", None
                
            elif response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 60))
                return False, f"Rate limit (429) - warten Sie {retry_after}s", None
                
            else:
                error_msg = f"Upload fehlgeschlagen: {response.status_code}"
                try:
                    error_detail = response.json()
                    error_msg += f" - {error_detail}"
                except:
                    error_msg += f" - {response.text[:200]}"
                
                # Bei komplettem Batch-Fehler alle Items als fehlgeschlagen markieren
                for idx, item in enumerate(items):
                    failed_item = item.copy()
                    failed_item['_error'] = error_msg
                    failed_item['_batch_index'] = batch_index
                    failed_item['_item_index'] = idx
                    self.failed_items.append(failed_item)
                
                return False, error_msg, None
                
        except requests.exceptions.Timeout:
            error_msg = "Upload Timeout - versuchen Sie kleinere Batches"
            # Bei Timeout alle Items als fehlgeschlagen markieren
            for idx, item in enumerate(items):
                failed_item = item.copy()
                failed_item['_error'] = error_msg
                failed_item['_batch_index'] = batch_index
                failed_item['_item_index'] = idx
                self.failed_items.append(failed_item)
            return False, error_msg, None
        except Exception as e:
            error_msg = f"Upload Fehler: {str(e)}"
            # Bei Exception alle Items als fehlgeschlagen markieren
            for idx, item in enumerate(items):
                failed_item = item.copy()
                failed_item['_error'] = error_msg
                failed_item['_batch_index'] = batch_index
                failed_item['_item_index'] = idx
                self.failed_items.append(failed_item)
            return False, error_msg, None

    def import_ris_to_group(self, ris_content: str, batch_size: int = 25, check_duplicates: bool = True) -> bool:
        """Hauptfunktion f√ºr RIS-Import mit umfassendem Fehlerhandling und Duplikatspr√ºfung"""
        
        # Erstelle Zeitstempel f√ºr Error-Log
        self.error_log_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.failed_items = []  # Liste f√ºr fehlgeschlagene Items
        
        logger.info("=" * 60)
        logger.info("üöÄ ZOTERO RIS IMPORT GESTARTET")
        logger.info("=" * 60)
        
        # Log-Datei Status pr√ºfen
        log_handlers = [h for h in logger.handlers if isinstance(h, logging.FileHandler)]
        if log_handlers:
            log_file = log_handlers[0].baseFilename
            logger.info(f"üìù Logs werden gespeichert in: {log_file}")
            # Sofort flushen
            for handler in log_handlers:
                handler.flush()
        else:
            logger.warning("‚ö†Ô∏è  Keine Log-Datei aktiv - nur Konsolen-Ausgabe")
        
        # 1. RIS-Inhalt validieren
        logger.info("üìã Schritt 1/5: RIS-Datei validieren...")
        is_valid, validation_msg = self.validate_ris_content(ris_content)
        if not is_valid:
            logger.error(f"‚ùå Validierung fehlgeschlagen: {validation_msg}")
            return False
        logger.info(f"‚úÖ {validation_msg}")
        
        # Zwischenspeichern in Log
        for handler in logger.handlers:
            if isinstance(handler, logging.FileHandler):
                handler.flush()
        
        # 2. RIS zu Zotero-JSON konvertieren
        logger.info("üîÑ Schritt 2/5: RIS zu Zotero-Format konvertieren...")
        logger.info("‚ö° Bei Server-√úberlastung wird automatisch Fallback-Parser verwendet...")
        logger.info("üìö Sammelb√§nde werden automatisch erkannt und korrekt verarbeitet...")
        
        conversion_start = time.time()
        items = self.convert_ris_with_fallback(ris_content)
        conversion_time = time.time() - conversion_start
        
        if not items:
            logger.error("‚ùå RIS-Konvertierung fehlgeschlagen")
            return False
        
        # Detaillierte Item-Analyse mit Sammelband-Erkennung
        item_types = {}
        has_abstracts = 0
        has_dois = 0
        has_creators = 0
        sammelb√§nde_count = 0
        
        for item in items:
            item_type = item.get('itemType', 'unknown')
            item_types[item_type] = item_types.get(item_type, 0) + 1
            
            if item.get('abstractNote'):
                has_abstracts += 1
            if item.get('DOI'):
                has_dois += 1
            if item.get('creators'):
                has_creators += 1
            if item.get('_was_sammelband', False):
                sammelb√§nde_count += 1
        
        logger.info(f"‚è±Ô∏è  Konvertierung dauerte: {conversion_time:.1f} Sekunden")
        logger.info(f"‚úÖ {len(items)} Items erfolgreich konvertiert")
        logger.info(f"üìö Davon Sammelb√§nde: {sammelb√§nde_count}")
        logger.info(f"üìä Item-Verteilung: {dict(sorted(item_types.items()))}")
        logger.info(f"üìà Qualit√§ts-Check: {has_abstracts} mit Abstract, {has_dois} mit DOI, {has_creators} mit Autoren")
        
        # Log flushen nach wichtigen Infos
        for handler in logger.handlers:
            if isinstance(handler, logging.FileHandler):
                handler.flush()
        
        # 3. Duplikatspr√ºfung (optional)
        if check_duplicates:
            logger.info("ÔøΩ Schritt 3/5: Duplikatspr√ºfung...")
            existing_items = self.get_existing_items()
            if existing_items is not None:
                items, duplicates = self.filter_duplicates(items, existing_items)
                if duplicates:
                    logger.info(f"üîÑ {len(duplicates)} Duplikate √ºbersprungen")
                    # Duplikate in separater Datei speichern
                    self._save_duplicates_report(duplicates)
                
                if not items:
                    logger.info("‚úÖ Alle Items waren bereits vorhanden - nichts zu importieren")
                    return True
            else:
                logger.warning("‚ö†Ô∏è  Duplikatspr√ºfung fehlgeschlagen - fahre ohne Pr√ºfung fort")
        else:
            logger.info("‚è≠Ô∏è  Schritt 3/5: Duplikatspr√ºfung √ºbersprungen (deaktiviert)")
        
        # 4. Library-Version abrufen
        logger.info("üîó Schritt 4/5: Library-Version abrufen...")
        library_version = self.get_library_version()
        if not library_version:
            logger.error("‚ùå Konnte Library-Version nicht abrufen")
            return False
        logger.info(f"‚úÖ Library-Version: {library_version}")
        
        # 5. Items in Batches hochladen
        total_batches = (len(items) + batch_size - 1) // batch_size
        logger.info("üì§ Schritt 5/5: Items zu Zotero hochladen...")
        logger.info(f"üìä Plane Upload: {len(items)} Items in {total_batches} Batches (√† {batch_size} Items)")
        logger.info("-" * 50)
        
        uploaded_count = 0
        failed_count = 0
        start_time = time.time()
        
        for i in range(0, len(items), batch_size):
            batch = items[i:i+batch_size]
            batch_num = (i // batch_size) + 1
            
            logger.info(f"üì¶ Batch {batch_num}/{total_batches}: Uploading {len(batch)} Items...")
            
            # Progress indicator
            progress = f"[{'‚ñà' * (batch_num * 20 // total_batches)}{'‚ñë' * (20 - batch_num * 20 // total_batches)}] {batch_num}/{total_batches}"
            logger.info(f"üìà Progress: {progress}")
            
            # Batch hochladen mit Retry
            batch_success = False
            for attempt in range(3):
                success, message, new_version = self.upload_items_batch(batch, library_version, batch_num)
                
                if success:
                    uploaded_count += len(batch)
                    logger.info(f"   ‚úÖ {message}")
                    if new_version:
                        library_version = new_version
                    batch_success = True
                    break
                else:
                    if "412" in message:  # Version veraltet
                        logger.warning(f"   ‚ö†Ô∏è  Versuch {attempt + 1}: {message}")
                        library_version = self.get_library_version()
                        if not library_version:
                            logger.error("   ‚ùå Konnte neue Library-Version nicht abrufen")
                            break
                    elif "429" in message:  # Rate limit
                        retry_after = 60
                        logger.warning(f"   ‚è≥ Versuch {attempt + 1}: {message}")
                        time.sleep(retry_after)
                    else:
                        logger.error(f"   ‚ùå Versuch {attempt + 1}: {message}")
                        if attempt == 2:  # Letzter Versuch
                            break
                        else:
                            time.sleep(5)
            
            if not batch_success:
                failed_count += len(batch)
                logger.error(f"   ‚ùå Batch {batch_num} komplett fehlgeschlagen")
            
            # Log nach jedem Batch flushen
            for handler in logger.handlers:
                if isinstance(handler, logging.FileHandler):
                    handler.flush()
            
            # Rate limiting zwischen Batches
            if i + batch_size < len(items):
                time.sleep(0.5)
            
            # Zwischenbericht alle 5 Batches
            if batch_num % 5 == 0 or batch_num == total_batches:
                elapsed = time.time() - start_time
                rate = uploaded_count / elapsed if elapsed > 0 else 0
                logger.info(f"üîÑ Zwischenbericht: {uploaded_count} hochgeladen, {failed_count} fehlgeschlagen ({rate:.1f} Items/s)")
        
        # 5. Finale Zusammenfassung
        total_time = time.time() - start_time
        success_rate = (uploaded_count / (uploaded_count + failed_count) * 100) if (uploaded_count + failed_count) > 0 else 0
        
        logger.info("=" * 60)
        logger.info("üéâ IMPORT ABGESCHLOSSEN!")
        logger.info("=" * 60)
        logger.info(f"üìä STATISTIKEN:")
        logger.info(f"   ‚úÖ Erfolgreich hochgeladen: {uploaded_count} Items")
        logger.info(f"   üìö Davon Sammelb√§nde: {sammelb√§nde_count}")
        logger.info(f"   ‚ùå Fehlgeschlagen: {failed_count} Items")
        if check_duplicates and 'duplicates' in locals():
            logger.info(f"   üîÑ Duplikate √ºbersprungen: {len(duplicates)} Items")
        logger.info(f"   üìà Erfolgsrate: {success_rate:.1f}%")
        logger.info(f"   ‚è±Ô∏è  Gesamtzeit: {total_time:.1f} Sekunden")
        logger.info(f"   üöÄ Durchschnitt: {(uploaded_count / total_time):.1f} Items/Sekunde")
        
        # 6. Error-Log schreiben wenn es Fehler gab
        if self.failed_items:
            error_log_file = f"zotero_import_{self.error_log_timestamp}_errors.log"
            try:
                with open(error_log_file, 'w', encoding='utf-8') as error_log:
                    error_log.write(f"Fehlerprotokoll des Zotero-Imports\n")
                    error_log.write(f"Zeitstempel: {self.error_log_timestamp}\n")
                    error_log.write(f"Gesamt: {len(items)} Items\n")
                    error_log.write(f"Erfolgreich: {uploaded_count}\n")
                    error_log.write(f"Fehlgeschlagen: {len(self.failed_items)}\n")
                    error_log.write("=" * 80 + "\n\n")
                    
                    for idx, failed_item in enumerate(self.failed_items, 1):
                        error_log.write(f"Fehlgeschlagenes Item #{idx}\n")
                        error_log.write(f"Batch: {failed_item.get('_batch_index', 'unbekannt')}\n")
                        error_log.write(f"Item-Index: {failed_item.get('_item_index', 'unbekannt')}\n")
                        error_log.write(f"Fehler: {failed_item.get('_error', 'Unbekannter Fehler')}\n")
                        error_log.write(f"Titel: {failed_item.get('title', 'Kein Titel')}\n")
                        error_log.write(f"Item-Typ: {failed_item.get('itemType', 'unbekannt')}\n")
                        
                        # Creators anzeigen
                        creators = failed_item.get('creators', [])
                        if creators:
                            error_log.write(f"Autoren/Herausgeber:\n")
                            for creator in creators[:3]:  # Nur erste 3
                                creator_name = creator.get('name') or f"{creator.get('lastName', '')}, {creator.get('firstName', '')}"
                                creator_type = creator.get('creatorType', 'unknown')
                                error_log.write(f"  - {creator_name} ({creator_type})\n")
                        
                        error_log.write("\nVollst√§ndiges Item (JSON):\n")
                        # Entferne interne Felder f√ºr saubere Ausgabe
                        clean_item = {k: v for k, v in failed_item.items() if not k.startswith('_')}
                        error_log.write(json.dumps(clean_item, indent=2, ensure_ascii=False))
                        error_log.write("\n\n" + "-" * 80 + "\n\n")
                
                logger.info(f"üìù Fehlerprotokoll gespeichert: {error_log_file}")
            except Exception as e:
                logger.error(f"‚ùå Konnte Fehlerprotokoll nicht schreiben: {e}")
        
        if failed_count == 0:
            logger.info("üéä PERFEKT! Alle Items erfolgreich importiert!")
        elif success_rate >= 90:
            logger.info("üéØ SEHR GUT! Import gr√∂√ütenteils erfolgreich!")
        elif success_rate >= 70:
            logger.info("‚ö†Ô∏è  AKZEPTABEL: Import teilweise erfolgreich!")
        else:
            logger.warning("üö® PROBLEMATISCH: Viele Fehler beim Import!")
        
        logger.info("=" * 60)
        
        # Finale Log-Speicherung
        for handler in logger.handlers:
            if isinstance(handler, logging.FileHandler):
                handler.flush()
        
        return failed_count == 0

    def import_ris_file(self, file_path: str, batch_size: int = 25, check_duplicates: bool = True) -> bool:
        """RIS-Datei laden und importieren mit Duplikatspr√ºfung"""
        
        if not os.path.exists(file_path):
            logger.error(f"‚ùå Datei nicht gefunden: {file_path}")
            return False
        
        # Dateigr√∂√üe pr√ºfen
        file_size = os.path.getsize(file_path)
        logger.info(f"RIS-Datei: {file_path} ({file_size/1024/1024:.1f} MB)")
        
        if file_size > 50 * 1024 * 1024:  # 50 MB
            logger.warning("‚ö†Ô∏è  Sehr gro√üe Datei - erw√§gen Sie eine Aufteilung")
        
        # Verschiedene Encodings versuchen
        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    ris_content = f.read()
                logger.info(f"‚úì Datei geladen mit Encoding: {encoding}")
                break
            except UnicodeDecodeError:
                logger.warning(f"Encoding {encoding} fehlgeschlagen")
                continue
        else:
            logger.error("‚ùå Konnte Datei mit keinem Encoding lesen")
            return False
        
        return self.import_ris_to_group(ris_content, batch_size, check_duplicates)


# Verwendungsbeispiel
def main():
    # Konfiguration
    GROUP_ID = "00000000" # Group ID
    API_KEY = "APIKEY"  # Hier API-Key einf√ºgen
    RIS_FILE = "FILENAME"
    BATCH_SIZE = 15  # Noch kleinere Batches bei Server-Problemen
    CHUNK_SIZE = 50  # RIS-Eintr√§ge pro Translation-Request
    CHECK_DUPLICATES = True  # Duplikatspr√ºfung aktivieren
    
    # TEST-MODUS: Setze auf True um nur Duplikatserkennung zu testen
    TEST_MODE = False  # ‚Üê Hier auf True setzen f√ºr Test ohne Upload
    
    # Importer erstellen
    importer = ZoteroImporter(GROUP_ID, API_KEY)
    importer.chunk_size = CHUNK_SIZE  # RIS-Chunk-Gr√∂√üe anpassen
    importer.use_fallback_parser = True  # Fallback-Parser aktivieren
    
    print("üîß Fallback-Parser ist aktiviert")
    print("‚ö° Intelligenter Fallback: Nach 2 fehlgeschlagenen Chunks wird sofort auf manuellen Parser umgeschaltet")
    print("üìä Erweiterte Logging: Detaillierte Progress-Updates und Statistiken")
    print(f"üîç Duplikatspr√ºfung: {'AKTIVIERT' if CHECK_DUPLICATES else 'DEAKTIVIERT'}")
    
    if CHECK_DUPLICATES:
        print("   - Normalisierte Feldvergleiche (Titel, DOI, ISBN, Autoren)")
        print("   - Pr√ºft auf DOI/ISBN-Matches")
        print("   - Pr√ºft auf Titel+Autor+Jahr-Matches")
        print("   - Spezielle Behandlung f√ºr Zeitschriftenartikel und Buchkapitel")
        print("   - Erstellt detaillierten Duplikate-Bericht")
    
    if TEST_MODE:
        print("\nüß™ TEST-MODUS AKTIVIERT")
        print("=" * 50)
        print("F√ºhre nur Duplikatserkennung durch (kein Upload)")
        print("Perfekt um zu testen ob identische Daten 100% erkannt werden")
        print("=" * 50)
        
        success = importer.test_ris_file_duplicates(RIS_FILE)
        
        if success:
            print("\n‚úÖ Duplikat-Test abgeschlossen! Siehe Log f√ºr Details.")
        else:
            print("\n‚ùå Duplikat-Test fehlgeschlagen. Siehe Log f√ºr Details.")
    else:
        print("\nüöÄ IMPORT-MODUS")
        print("F√ºhre vollst√§ndigen Import durch")
        
        # Import ausf√ºhren
        success = importer.import_ris_file(RIS_FILE, BATCH_SIZE, CHECK_DUPLICATES)
        
        if success:
            print("\nüéâ Import erfolgreich abgeschlossen!")
        else:
            print("\n‚ùå Import mit Fehlern beendet. Siehe Log f√ºr Details.")

# Script ausf√ºhren
if __name__ == "__main__":
    main()
